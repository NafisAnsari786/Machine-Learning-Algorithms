{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Find out the correlation between the test scores of math and CS i.e.( math is x and CS is y )\n",
    "\n",
    "Using this find the values of m and b using gradient descent and then we have to compare the cost between each iteration and when it is certain thresold (threshold calculated using : math.isclose(cost, cost_previous, rel_tol=1e-20) ) then we have to break the loop and show how many iterations it took to find the value of m and b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m98.918, b1.3980000000000001, cost25995.5, iterations0\n",
      "m-9169.587968000002, b-129.386408, cost226830541.704252, iterations1\n",
      "m859278.7790833284, b12125.168845408005, cost1991451648098.9502, iterations2\n",
      "m-80513336.3321037, b-1136112.3853868274, cost1.7483899730698202e+16, iterations3\n",
      "m7544007292.656351, b106452428.60001147, cost1.53499458603139e+20, iterations4\n",
      "m-706864823521.7386, b-9974470361.562872, cost1.347644641892262e+24, iterations5\n",
      "m66232422560018.44, b934596423272.1094, cost1.183161228937249e+28, iterations6\n",
      "m-6205901966243919.0, b-87570612045376.58, cost1.0387534296093866e+32, iterations7\n",
      "m5.814858905354231e+17, b8205265827097408.0, cost9.11970964848519e+35, iterations8\n",
      "m-5.448456046050343e+19, b-7.688239892447695e+17, cost8.006626183073001e+39, iterations9\n",
      "m5.105140772789593e+21, b7.203792526577272e+19, cost7.029397349960405e+43, iterations10\n",
      "m-4.783458302630841e+23, b-6.74987090568644e+21, cost6.171441750096236e+47, iterations11\n",
      "m4.482045520657572e+25, b6.324551557439078e+23, cost5.418201785825266e+51, iterations12\n",
      "m-4.1996252038400877e+27, b-5.926032210335615e+25, cost4.756896650845377e+55, iterations13\n",
      "m3.9350006088606955e+29, b5.552624156669068e+27, cost4.176305468360736e+59, iterations14\n",
      "m-3.6870504009680314e+31, b-5.202745096702534e+29, cost3.666576897768033e+63, iterations15\n",
      "m3.4547239023717715e+33, b4.874912433745606e+31, cost3.21906197932479e+67, iterations16\n",
      "m-3.237036640043024e+35, b-4.567736991717981e+33, cost2.826167380545692e+71, iterations17\n",
      "m3.0330661740543987e+37, b4.279917128578644e+35, cost2.4812265542448012e+75, iterations18\n",
      "m-2.8419481887825394e+39, b-4.0102332206766067e+37, cost2.178386622062279e+79, iterations19\n",
      "m2.662872830409765e+41, b3.757542494650841e+39, cost1.9125090641407522e+83, iterations20\n",
      "m-2.4950812752051536e+43, b-3.520774184979868e+41, cost1.6790825298760771e+87, iterations21\n",
      "m2.3378625140433015e+45, b3.298924996661283e+43, cost1.4741462903349465e+91, iterations22\n",
      "m-2.190550339535319e+47, b-3.09105485379457e+45, cost1.2942230334971485e+95, iterations23\n",
      "m2.052520523005154e+49, b2.8962829160519996e+47, cost1.136259862007301e+99, iterations24\n",
      "m-1.923188169348813e+51, b-2.7137838461575763e+49, cost9.975764922991505e+102, iterations25\n",
      "m1.8020052385678148e+53, b2.5427843125576694e+51, cost8.758197761468425e+106, iterations26\n",
      "m-1.6884582234744865e+55, b-2.382559712463536e+53, cost7.68923772975077e+110, iterations27\n",
      "m1.5820659737284843e+57, b2.2324311013797718e+55, cost6.750746954440749e+114, iterations28\n",
      "m-1.4823776569839887e+59, b-2.0917623161077335e+57, cost5.926801335139393e+118, iterations29\n",
      "m1.3889708485080324e+61, b1.9599572790327534e+59, cost5.203420347892467e+122, iterations30\n",
      "m-1.3014497411748039e+63, b-1.8364574722722103e+61, cost4.5683298268043856e+126, iterations31\n",
      "m1.219443468250852e+65, b1.7207395709812706e+63, cost4.0107536987519e+130, iterations32\n",
      "m-1.1426045318640812e+67, b-1.6123132257875217e+65, cost3.521231137398967e+134, iterations33\n",
      "m1.0706073304972364e+69, b1.510718984957695e+67, cost3.091456034023349e+138, iterations34\n",
      "m-1.0031467792662015e+71, b-1.4155263474917225e+69, cost2.7141360613319263e+142, iterations35\n",
      "m9.399370171366024e+72, b1.3263319388942262e+71, cost2.3828689388913194e+146, iterations36\n",
      "m-8.807101956006055e+74, b-1.2427578018933375e+73, cost2.092033800673429e+150, iterations37\n",
      "m8.252153436809802e+76, b1.164449794863854e+75, cost1.8366958214648692e+154, iterations38\n",
      "m-7.732173044529361e+78, b-1.0910760911681242e+77, cost1.6125224838626373e+158, iterations39\n",
      "m7.244957385772907e+80, b1.0223257730556751e+79, cost1.4157100650932492e+162, iterations40\n",
      "m-6.788441906224863e+82, b-9.579075141633148e+80, cost1.2429190963002183e+166, iterations41\n",
      "m6.360692142190374e+84, b8.975483450328412e+82, cost1.0912176991875778e+170, iterations42\n",
      "m-5.959895523392907e+86, b-8.409924964153124e+84, cost9.580318385683667e+173, iterations43\n",
      "m5.584353692289688e+88, b7.880003154604224e+86, cost8.41101646714505e+177, iterations44\n",
      "m-5.232475307358434e+90, b-7.3834725019839e+88, cost7.384430784294516e+181, iterations45\n",
      "m4.9027692998596465e+92, b6.918228980111427e+90, cost6.483142462155412e+185, iterations46\n",
      "m-4.5938385555002596e+94, b-6.482301140607395e+92, cost5.691858643186954e+189, iterations47\n",
      "m4.304373994225029e+96, b6.073841758970393e+94, cost4.997153001516968e+193, iterations48\n",
      "m-4.033149023049007e+98, b-5.69112000704673e+96, cost4.387237928064238e+197, iterations49\n",
      "m3.779014338425252e+100, b5.332514118724415e+98, cost3.8517645210387573e+201, iterations50\n",
      "m-3.540893056123038e+102, b-4.996504517772638e+100, cost3.3816469881037407e+205, iterations51\n",
      "m3.317776147979637e+104, b4.681667378706207e+102, cost2.968908480694744e+209, iterations52\n",
      "m-3.108718166189119e+106, b-4.386668593389475e+104, cost2.6065457446473034e+213, iterations53\n",
      "m2.912833237010043e+108, b4.110258117813446e+106, cost2.2884102905553707e+217, iterations54\n",
      "m-2.7292913069155488e+110, b-3.8512646750908465e+108, cost2.0091040675859377e+221, iterations55\n",
      "m2.557314625279079e+112, b3.608590792223282e+110, cost1.7638878705665807e+225, iterations56\n",
      "m-2.3961744486912823e+114, b-3.381208149609062e+112, cost1.548600926217987e+229, iterations57\n",
      "m2.2451879529427813e+116, b3.1681532235853846e+114, cost1.3595902940887543e+233, iterations58\n",
      "m-2.103715339587469e+118, b-2.968523203540419e+116, cost1.1936488842834045e+237, iterations59\n",
      "m1.9711571248255345e+120, b2.7814721662941635e+118, cost1.0479610402823345e+241, iterations60\n",
      "m-1.8469515992179773e+122, b-2.606207491537232e+120, cost9.200547635152693e+244, iterations61\n",
      "m1.7305724474682703e+124, b2.441986503138153e+122, cost8.077597690454974e+248, iterations62\n",
      "m-1.621526518185095e+126, b-2.288113322086855e+124, cost7.091706606631865e+252, iterations63\n",
      "m1.5193517341756278e+128, b2.143935917738836e+126, cost6.226146005510373e+256, iterations64\n",
      "m-1.4236151344143378e+130, b-2.0088433448647983e+128, cost5.466229249484387e+260, iterations65\n",
      "m1.3339110393902257e+132, b1.8822631547978803e+130, cost4.799062241950962e+264, iterations66\n",
      "m-1.2498593320582444e+134, b-1.7636589697083224e+132, cost4.2133246431791814e+268, iterations67\n",
      "m1.1711038471105164e+136, b1.6525282097266731e+134, cost3.699077789331652e+272, iterations68\n",
      "m-1.0973108617419513e+138, b-1.5483999632843285e+136, cost3.24759605545184e+276, iterations69\n",
      "m1.028167681514955e+140, b1.4508329916470608e+138, cost2.8512188010222855e+280, iterations70\n",
      "m-9.633813153308937e+141, b-1.359413859185839e+140, cost2.5032203859392543e+284, iterations71\n",
      "m9.026772338935685e+143, b1.2737551814620527e+142, cost2.1976960513641373e+288, iterations72\n",
      "m-8.457982064037382e+145, b-1.1934939837034794e+144, cost1.9294617291035155e+292, iterations73\n",
      "m7.925032105552447e+147, b1.1182901627151002e+146, cost1.6939660795060014e+296, iterations74\n",
      "m-7.425664112138921e+149, b-1.0478250457071987e+148, cost1.487213265354683e+300, iterations75\n",
      "m6.957762034512808e+151, b9.818000399338293e+149, cost1.3056951513999337e+304, iterations76\n",
      "m-6.519343158784951e+153, b-9.199353674195595e+151, costinf, iterations77\n",
      "Using Gradient Descent function: coef6.1085497048006155e+155 intercept8.619688794129578e+153\n",
      "Using sklearn: coef[1.01773624] intercept1.9152193111568891\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nafis Ansari\\AppData\\Local\\Temp\\ipykernel_7408\\2050145214.py:22: RuntimeWarning: overflow encountered in scalar add\n",
      "  cost=(1/2)*sum([val**2 for val in (y-y_predicted)])\n",
      "C:\\Users\\Nafis Ansari\\AppData\\Local\\Temp\\ipykernel_7408\\2050145214.py:22: RuntimeWarning: overflow encountered in scalar power\n",
      "  cost=(1/2)*sum([val**2 for val in (y-y_predicted)])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import math\n",
    "\n",
    "def prediction_using_sklearn():\n",
    "    df=pd.read_csv(r\"F:\\Machine Learning all Algorithms\\Day 1\\Gradient Descent\\test_scores.csv\")\n",
    "    reg=LinearRegression()\n",
    "    reg.fit(df[['math']], df['cs'])\n",
    "    return reg.coef_, reg.intercept_\n",
    "\n",
    "def gradient_descent(x,y):\n",
    "    m_curr=0\n",
    "    b_curr=0\n",
    "    iterations=1000\n",
    "    n=len(x)\n",
    "    learning_rate=0.01\n",
    "    cost_previous=0\n",
    "\n",
    "    for i in range(iterations):\n",
    "        y_predicted=m_curr * x + b_curr         # y=mx+b\n",
    "        cost=(1/2)*sum([val**2 for val in (y-y_predicted)])\n",
    "        md = -(2/n)*sum(x*(y-y_predicted))      \n",
    "        bd = -(2/n)*sum(y-y_predicted)\n",
    "        m_curr = m_curr-learning_rate*md\n",
    "        b_curr = b_curr-learning_rate*bd\n",
    "        if math.isclose(cost, cost_previous, rel_tol=1e-20):\n",
    "            break\n",
    "        cost_previous=cost\n",
    "        print(\"m{}, b{}, cost{}, iterations{}\".format(m_curr, b_curr, cost, i))\n",
    "\n",
    "    return m_curr, b_curr\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    df=pd.read_csv(r\"F:\\Machine Learning all Algorithms\\Day 1\\Gradient Descent\\test_scores.csv\")\n",
    "    x=np.array(df.math)\n",
    "    y=np.array(df.cs)\n",
    "\n",
    "    m,b=gradient_descent(x,y)\n",
    "    print(\"Using Gradient Descent function: coef{} intercept{}\".format(m,b))\n",
    "\n",
    "    m_sklearn, b_sklearn=prediction_using_sklearn()\n",
    "    print(\"Using sklearn: coef{} intercept{}\".format(m_sklearn, b_sklearn))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
